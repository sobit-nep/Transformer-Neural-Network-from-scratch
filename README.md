# Transformer-Neural-Networks-from-scratch

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![GitHub Stars](https://img.shields.io/github/stars/sobit-nep/Transformer-Neural-Network-from-scratch.svg?style=flat&logo=github&colorB=blue&label=stars)](https://github.com/sobit-nep/Transformer-Neural-Network-from-scratch/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/sobit-nep/Transformer-Neural-Network-from-scratch.svg?style=flat&logo=github&colorB=blue&label=forks)](https://github.com/sobit-nep/Transformer-Neural-Network-from-scratch/network/members)

A simple implementation of the Transformer architecture from scratch using [TensorFlow/PyTorch] for educational purposes. This repository serves as a learning resource for understanding the core components of transformers, such as self-attention and position-wise feedforward networks.

## Table of Contents
- [Introduction](#introduction)
- [Implementation Details](#implementation-details)

## Introduction
The Transformer model, introduced by Vaswani et al. in the paper "Attention is All You Need," has become a cornerstone in natural language processing and beyond. This repository aims to provide a clear and concise implementation of the Transformer architecture from scratch. It serves as an educational resource for deep learning enthusiasts who want to grasp the fundamentals of transformers.

## Implementation Details
- **Language:** Python
- **Framework:** [TensorFlow/PyTorch]
- **Key Components:**
  - Self-Attention Mechanism
  - Multi-Head Attention 
  - Positional Encoding
  - Position-wise Feedforward Networks
  - Layer Normalization
  

